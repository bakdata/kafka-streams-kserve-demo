apiVersion: serving.kubeflow.org/v1beta1
kind: InferenceService
metadata:
  name: mlserver-translator
  annotations:
    autoscaling.knative.dev/target: "1"
spec:
  predictor:
    containerConcurrency: 1
    maxReplicas: 5
    minReplicas: 0
    containers:
      - name: mlserver-translator
        image: bakdata/kserve-demo-mlserver-translator:latest
        env:
          - name: MLSERVER_MODEL_PARALLEL_WORKERS  # Disable MLServer parallel inference
            value: "0"
        resources:
          requests:
            memory: 1G
            cpu: "1"
          limits:
            memory: 2G
            cpu: "1"
